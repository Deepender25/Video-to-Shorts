import json
import re
import time
import google.generativeai as genai
from config import GEMINI_API_KEY, GEMINI_MODEL, CHUNK_MINUTES, MAX_RETRIES


# ‚îÄ‚îÄ Language-specific prompt blocks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_LANG_INSTRUCTIONS = {
    "hi": """
## Title Language: HINGLISH (Hindi + English in Roman Script)

The transcript is in HINDI. You MUST write all titles in **Hinglish** ‚Äî Hindi words in Roman/English letters, mixed with English words, exactly how Indian creators write on Instagram Reels & YouTube Shorts.

### Great Hinglish Title Examples:
- "Isse Zyada Savage Reply Nahi Dekha Hoga üî•"
- "Ye Baat Sunke Sabke Hosh Ud Gaye üò±"
- "Pyaar Ka Asli Matlab Kya Hai? üíî"
- "Isne Sabki Band Baja Di üíÄ"
- "Ye Reality Check Zaroor Suno üéØ"
- "Ek Kahani Jo Dil Chhu Legi ‚ù§Ô∏è"

### BAD titles (NEVER write like this):
- "A Discussion About Love" ‚Üê too English, too boring
- "‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§" ‚Üê NO Devanagari script ever
- "Important conversation" ‚Üê generic, zero emotion
""",
    "en": """
## Title Language: ENGLISH

Write catchy, scroll-stopping English titles for Instagram Reels & YouTube Shorts.

### Great English Title Examples:
- "He DESTROYED Her Ego in 10 Seconds üíÄ"
- "This Life Advice Hits DIFFERENT at 3AM üéØ"
- "Nobody Talks About This But It's SO True üò±"
- "The Most Savage Comeback I've Ever Heard üî•"
- "This Story Will Change How You Think Forever üí°"
- "Wait For The Plot Twist at The End üòÆ"

### BAD titles (NEVER write like this):
- "A conversation about life" ‚Üê boring, generic
- "Speaker talks about success" ‚Üê description, not a hook
- "Discussion on relationships" ‚Üê nobody clicks this
""",
}


SYSTEM_PROMPT = """You are a world-class short-form video editor who creates VIRAL YouTube Shorts and Instagram Reels. You specialize in **storytelling** ‚Äî turning long videos into compelling mini-stories that keep viewers hooked from first to last second.

## Your Superpower: Storytelling Through Compilation

You don't just cut random interesting moments. You CREATE STORIES by:

1. **Compiling multiple segments** from different parts of the video into one cohesive short
2. **Building narrative arcs**: Hook ‚Üí Context ‚Üí Build-up ‚Üí Climax/Payoff
3. **Connecting the dots**: Taking related moments scattered across the video and weaving them into one powerful short

## Two Types of Shorts You Create

### Type 1: Single-Segment Shorts (15‚Äì60 seconds)
A continuous clip that naturally tells a complete story on its own.
- Use when a single moment is already powerful and self-contained
- Still needs a strong hook and natural conclusion

### Type 2: Compiled Shorts (30‚Äì150 seconds) ‚≠ê PREFERRED
Multiple segments stitched together to create a BETTER story than any single segment could tell.
- **Combine 2-4 segments** from different parts of the video
- Each segment must flow naturally into the next (same topic/theme)
- The combined short must feel like ONE cohesive narrative without feeling jumpy or disjointed
- Total duration of all segments combined: 30 to 150 seconds

**Example**: For a video about someone's life story:
- Segment 1 (0:30‚Äì0:55): The struggle/problem they faced
- Segment 2 (3:15‚Äì3:45): The turning point/realization
- Segment 3 (7:00‚Äì7:20): The result/transformation
‚Üí Together = a 75-second mini-documentary that tells a complete arc

## What Makes Content Go VIRAL

1. **Irresistible hook** ‚Äî first 3 seconds must STOP the scroll
2. **Emotional journey** ‚Äî take viewers through feelings (curiosity ‚Üí shock, sadness ‚Üí hope, confusion ‚Üí clarity)
3. **Payoff at the end** ‚Äî every short needs a satisfying conclusion or punchline
4. **Relatability** ‚Äî moments viewers can see themselves in
5. **Curiosity gap** ‚Äî create the NEED to watch till the end

## What to AVOID

- Greetings, intros, "hey guys", "namaste", "welcome"
- Outros, subscribe reminders, "like share subscribe"
- Mid-sentence cuts ‚Äî always start AND end at natural pauses
- Segments that need visual context the audio can't convey
- Filler, repetition, or low-energy moments
- Shorts where the segments feel disconnected, jarring, or junky when combined
- Boring talking-head moments without a clear point; ensure the content is highly interesting from the viewer's POV

## STRICT RULES

1. Each individual segment must be at least 8 seconds long
2. Total short duration (all segments combined) must be 15‚Äì150 seconds (up to 2.5 minutes)
3. Timestamps MUST come DIRECTLY from the transcript ‚Äî NEVER invent timestamps
4. Use the EXACT start time from the first line of each segment
5. Use the EXACT end time from the last line of each segment
6. Segments within a short must be from the SAME topic/theme and flow SEAMLESSLY like a natural conversation
7. Shorts MUST NOT have overlapping segments with other shorts
8. Only select solid "8/10" moments or better; skip mediocre or incomplete thoughts to maintain high consistent quality. The output MUST be interesting and valuable to the viewer.
9. PREFER compiled multi-segment shorts over single-segment cuts
10. Every timestamp must be a **number in SECONDS** (float), NOT "MM:SS"

## Timestamp Conversion

Transcript uses [MM:SS‚ÄìMM:SS]. Convert to seconds:
- "0:00" ‚Üí 0.0, "0:45" ‚Üí 45.0, "1:30" ‚Üí 90.0
- "2:15" ‚Üí 135.0, "5:00" ‚Üí 300.0, "12:45" ‚Üí 765.0

## Title Rules

- 5‚Äì12 words, catchy, scroll-stopping
- Use power words: DESTROYED, SAVAGE, INSANE, SHOCKING, NOBODY, TRUTH
- Add 1 emoji at the end
- Create a CURIOSITY GAP ‚Äî make people NEED to watch
- NEVER write boring descriptive titles

{lang_instructions}

## Output Format (STRICT JSON, nothing else)

{{
  "clips": [
    {{
      "title": "<viral title>",
      "hook": "<exact opening line from transcript>",
      "segments": [
        {{"start": <seconds>, "end": <seconds>}},
        {{"start": <seconds>, "end": <seconds>}}
      ]
    }}
  ]
}}

Each clip MUST have a "segments" array (even single-segment clips should have exactly one entry in the array)."""


def _mmss_to_seconds(mmss: str) -> float:
    """Convert MM:SS string to seconds."""
    parts = mmss.strip().split(":")
    if len(parts) == 2:
        return int(parts[0]) * 60 + int(parts[1])
    return 0.0


def _chunk_transcript(formatted_text: str, chunk_minutes: int = CHUNK_MINUTES) -> list[str]:
    """
    Split a formatted transcript into time-based chunks.
    Each chunk covers roughly `chunk_minutes` worth of content.
    If the last chunk is shorter than MIN_LAST_CHUNK_MINUTES, it is merged
    into the previous chunk so Gemini always gets substantial content.
    """
    MIN_LAST_CHUNK_MINUTES = 2.5  # merge last chunk if shorter than this

    lines = formatted_text.strip().split("\n")
    if not lines:
        return []

    chunks = []
    current_chunk = []
    chunk_start_time = None
    chunk_start_times = []

    for line in lines:
        try:
            time_part = line.split("‚Äì")[0].replace("[", "").strip()
            start_time = _mmss_to_seconds(time_part)
        except (ValueError, IndexError):
            current_chunk.append(line)
            continue

        if chunk_start_time is None:
            chunk_start_time = start_time

        if start_time - chunk_start_time >= chunk_minutes * 60 and current_chunk:
            chunks.append("\n".join(current_chunk))
            chunk_start_times.append(chunk_start_time)
            current_chunk = [line]
            chunk_start_time = start_time
        else:
            current_chunk.append(line)

    if current_chunk:
        chunks.append("\n".join(current_chunk))
        chunk_start_times.append(chunk_start_time or 0)

    # Merge the last chunk into the previous one if it's too short.
    if len(chunks) >= 2:
        last_start = chunk_start_times[-1]
        # Find the last timestamp in the last chunk to measure its duration
        last_chunk_end = last_start
        for line in chunks[-1].strip().split("\n"):
            try:
                time_part = line.split("‚Äì")[0].replace("[", "").strip()
                last_chunk_end = _mmss_to_seconds(time_part)
            except (ValueError, IndexError):
                continue

        last_chunk_duration = last_chunk_end - last_start
        if last_chunk_duration < MIN_LAST_CHUNK_MINUTES * 60:
            print(f"  Merging short last chunk ({last_chunk_duration:.0f}s) into previous chunk")
            chunks[-2] = chunks[-2] + "\n" + chunks[-1]
            chunks.pop()
            chunk_start_times.pop()

    return chunks


def _strip_think_blocks(text: str) -> str:
    """Remove <think>...</think> blocks that some models prepend."""
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()


def _repair_json(text: str) -> str:
    """Fix common JSON issues from LLM output."""
    text = re.sub(r",\s*([}\]])", r"\1", text)  # trailing commas
    text = text.replace("\ufeff", "").replace("\u200b", "")  # invisible chars
    return text


def _extract_json(text: str) -> str | None:
    """
    Robustly extract a JSON object from Gemini's response.
    Handles markdown fences, thinking blocks, and multiple brace positions.
    """
    text = _strip_think_blocks(text)

    # Strategy 1: JSON inside markdown code fences
    fence_match = re.search(r"```(?:json)?\s*\n?(.*?)```", text, re.DOTALL)
    if fence_match:
        candidate = _repair_json(fence_match.group(1).strip())
        try:
            json.loads(candidate)
            return candidate
        except json.JSONDecodeError:
            pass

    # Strategy 2: Find balanced { } blocks, prefer ones with "clips" key
    pos = 0
    while pos < len(text):
        brace_start = text.find("{", pos)
        if brace_start == -1:
            break

        depth = 0
        for i in range(brace_start, len(text)):
            if text[i] == "{":
                depth += 1
            elif text[i] == "}":
                depth -= 1
                if depth == 0:
                    candidate = _repair_json(text[brace_start:i + 1])
                    try:
                        data = json.loads(candidate)
                        if isinstance(data, dict) and "clips" in data:
                            return candidate
                    except json.JSONDecodeError:
                        pass
                    break

        pos = brace_start + 1

    # Strategy 2.5: Find balanced [ ] blocks (list of clips)
    pos = 0
    while pos < len(text):
        bracket_start = text.find("[", pos)
        if bracket_start == -1:
            break

        depth = 0
        for i in range(bracket_start, len(text)):
            if text[i] == "[":
                depth += 1
            elif text[i] == "]":
                depth -= 1
                if depth == 0:
                    candidate = _repair_json(text[bracket_start:i + 1])
                    try:
                        data = json.loads(candidate)
                        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
                            return candidate
                    except json.JSONDecodeError:
                        pass
                    break

        pos = bracket_start + 1

    # Strategy 3: whole text directly
    repaired = _repair_json(text)
    try:
        json.loads(repaired)
        return repaired
    except json.JSONDecodeError:
        pass

    preview = text[:500].replace("\n", " ")
    print(f"  ‚úó Could not extract JSON. Response preview: {preview}")
    return None


def _normalize_clips(clips: list[dict]) -> list[dict]:
    """
    Normalize clip format: ensure every clip has a `segments` array.
    Handles both old format {start, end} and new format {segments: [...]}.
    """
    normalized = []
    for clip in clips:
        if "segments" in clip and isinstance(clip["segments"], list):
            # New format ‚Äî validate each segment has start/end
            valid_segs = []
            for seg in clip["segments"]:
                try:
                    valid_segs.append({
                        "start": float(seg["start"]),
                        "end": float(seg["end"]),
                    })
                except (KeyError, ValueError, TypeError):
                    continue
            if valid_segs:
                normalized.append({
                    "title": str(clip.get("title", "Untitled")),
                    "hook": str(clip.get("hook", "")),
                    "segments": valid_segs,
                })
        elif "start" in clip and "end" in clip:
            # Old format ‚Äî convert to segments array
            try:
                normalized.append({
                    "title": str(clip.get("title", "Untitled")),
                    "hook": str(clip.get("hook", "")),
                    "segments": [{
                        "start": float(clip["start"]),
                        "end": float(clip["end"]),
                    }],
                })
            except (ValueError, TypeError):
                continue

    return normalized


def _get_lang_instructions(subtitle_lang: str) -> str:
    """Get the language-specific prompt block."""
    base_lang = subtitle_lang.split("-")[0].lower()
    return _LANG_INSTRUCTIONS.get(base_lang, _LANG_INSTRUCTIONS["en"])


def _call_gemini(
    transcript_text: str,
    chunk_index: int,
    total_chunks: int,
    subtitle_lang: str = "en",
) -> list[dict]:
    """
    Send a single transcript chunk to Gemini and parse the response.
    Returns normalized clips with segments arrays.
    """
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_MODEL)

    lang_instructions = _get_lang_instructions(subtitle_lang)
    system_prompt = SYSTEM_PROMPT.format(lang_instructions=lang_instructions)

    chunk_info = ""
    if total_chunks > 1:
        chunk_info = (
            f"\n\nNOTE: This is chunk {chunk_index + 1} of {total_chunks} "
            "from a longer video. Focus only on timestamps in this chunk. "
            "Prefer compiled multi-segment shorts that tell a story arc."
        )

    user_prompt = f"""Analyze this transcript and create the most VIRAL short-form video content possible. Prefer COMPILED shorts that combine multiple segments into a storytelling arc. Return ONLY valid JSON.{chunk_info}

DIALOGUE TRANSCRIPT:
{transcript_text}"""

    for attempt in range(MAX_RETRIES):
        try:
            print(f"  Gemini attempt {attempt + 1}/{MAX_RETRIES}...")

            response = model.generate_content(
                [
                    {"role": "user", "parts": [{"text": system_prompt}]},
                    {"role": "model", "parts": [{"text": "I will analyze the transcript for storytelling arcs, find the best segments to compile into viral shorts, and return only valid JSON with the segments array format."}]},
                    {"role": "user", "parts": [{"text": user_prompt}]},
                ],
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=4096,
                    response_mime_type="application/json",
                ),
            )

            raw = response.text.strip()
            print(f"  Gemini response: {len(raw)} chars")

            json_str = _extract_json(raw)

            if not json_str:
                try:
                    json.loads(raw)
                    json_str = raw
                except json.JSONDecodeError:
                    raise json.JSONDecodeError(
                        "Could not extract valid JSON from response",
                        raw[:300], 0
                    )

            data = json.loads(json_str)
            clips = []
            if isinstance(data, dict):
                clips = data.get("clips", [])
            elif isinstance(data, list):
                clips = data
            else:
                raise ValueError("JSON must be a list of clips or an object containing a 'clips' array.")

            # Normalize to segments format
            normalized = _normalize_clips(clips)

            if normalized:
                # Log what we got
                for c in normalized:
                    seg_count = len(c["segments"])
                    total_dur = sum(s["end"] - s["start"] for s in c["segments"])
                    print(f"    ‚Üí \"{c['title']}\" ({seg_count} segment(s), {total_dur:.0f}s)")
                return normalized

            print("  ‚ö† Gemini returned valid JSON but no usable clips")
            return []

        except json.JSONDecodeError as e:
            if attempt < MAX_RETRIES - 1:
                wait = 2 ** (attempt + 1)
                print(f"  ‚ü≥ Invalid JSON (attempt {attempt + 1}): {str(e)[:100]}")
                print(f"    Waiting {wait}s before retry...")
                time.sleep(wait)
                continue
            print(f"  ‚úó Invalid JSON after {MAX_RETRIES} attempts")
            raise ValueError(
                f"Gemini returned invalid JSON after {MAX_RETRIES} attempts. "
                "Try again in a moment."
            )

        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait = 2 ** (attempt + 1)
                print(f"  ‚ü≥ Gemini API error (attempt {attempt + 1}): {e}")
                print(f"    Waiting {wait}s before retry...")
                time.sleep(wait)
                continue
            raise RuntimeError(
                f"Gemini API error after {MAX_RETRIES} attempts: {e}"
            )


def segment_transcript(
    formatted_text: str,
    subtitle_lang: str = "en",
) -> list[dict]:
    """
    Segment a formatted transcript into compiled shorts using Gemini API.

    Returns:
        list of {title, hook, segments: [{start, end}, ...]}
    """
    chunks = _chunk_transcript(formatted_text)
    all_clips = []
    total = len(chunks)

    lang_label = "Hinglish" if subtitle_lang.startswith("hi") else "English"
    print(f"  Sending {total} chunk(s) to Gemini (titles in {lang_label})...")

    for i, chunk in enumerate(chunks):
        print(f"  Processing chunk {i + 1}/{total}...")
        clips = _call_gemini(chunk, i, total, subtitle_lang=subtitle_lang)
        all_clips.extend(clips)
        print(f"  ‚úì Got {len(clips)} shorts from chunk {i + 1}")

    return all_clips
